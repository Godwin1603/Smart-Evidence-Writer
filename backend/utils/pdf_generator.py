# pdf_generator.py
from reportlab.lib.pagesizes import letter, A4
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Frame, PageTemplate, ListFlowable, Table, TableStyle, PageBreak, Image
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.pdfbase import pdfmetrics
from reportlab.lib.units import inch, mm
from reportlab.lib import colors
from reportlab.lib.enums import TA_CENTER, TA_LEFT, TA_JUSTIFY, TA_RIGHT
from reportlab.pdfgen import canvas
import io
import os
import re
from datetime import datetime
import base64
import logging

# Configure logging
logger = logging.getLogger(__name__)

# Multilingual text dictionaries
REPORT_TEXTS = {
    'en': {
        'title': 'ALFA LABS - EVIDENCE ANALYSIS REPORT',
        'header_system': 'Automated Evidence Processing System',
        'header_police': 'Tamil Nadu Police - Certified Analysis',
        'classification': 'RESTRICTED',
        'case_info': '1. CASE INFORMATION',
        'case_number': 'Case Number:',
        'investigating_officer': 'Investigating Officer:',
        'station': 'Station:',
        'location_incident': 'Location of Incident:',
        'date_incident': 'Date of Incident:',
        'time_incident': 'Time of Incident:',
        'executive_summary': '2. EXECUTIVE SUMMARY',
        'detailed_analysis': '3. DETAILED EVIDENCE ANALYSIS',
        'chronological_timeline': '4. CHRONOLOGICAL TIMELINE',
        'key_evidence_findings': '5. KEY EVIDENCE FINDINGS',
        'critical_identifiers': '6. CRITICAL IDENTIFIERS',
        'investigative_recommendations': '7. INVESTIGATIVE RECOMMENDATIONS',
        'analytical_intelligence': '8. ANALYTICAL INTELLIGENCE',
        'visual_evidence': '9. VISUAL EVIDENCE SNAPSHOTS',
        'confidence_metrics': '10. CONFIDENCE METRICS',
        'report_generated_by': 'Report Generated By:',
        'system_version': 'System Version:',
        'analysis_confidence': 'Analysis Confidence:',
        'computer_generated_note': 'This report is computer-generated and should be verified by investigating officers.',
        'investigating_officer_signature': 'Investigating Officer Signature:',
        'reviewing_officer_signature': 'Reviewing Officer Signature:',
        'date': 'Date:',
        'footer_text': 'Alfa Labs Evidence Analysis System - Confidential',
        'header_text': 'Automated Evidence Analysis Report',
        'no_summary': 'No executive summary available.',
        'no_analysis': 'No detailed analysis available.',
        'no_timeline': 'No explicit timeline could be extracted from the analysis.',
        'no_findings': 'No specific key findings could be automatically extracted.',
        'no_identifiers': 'No critical identifiers (vehicle plates, phone numbers) were automatically detected.',
        'vehicle_plates': 'Vehicle Plates:',
        'phone_numbers': 'Phone Numbers:',
        'event': 'Event',
        'finding': 'Finding',
        'analysis_point': 'Analysis Point',
        'risk_level': 'Risk Level:',
        'confidence_score': 'Confidence Score:',
        'analytical_insights': 'Analytical Insights:',
        'key_frames': 'Key Visual Evidence:',
        'risk_assessment': 'Risk Assessment',
        'threat_level': 'Threat Level',
        'recommended_response': 'Recommended Response'
    },
    'ta': {
        'title': 'роЕро▓рпНрокро╛ ро▓рпЗрокрпНро╕рпН - роЖродро╛ро░ рокроХрпБрокрпНрокро╛ропрпНро╡рпБ роЕро▒ро┐роХрпНроХрпИ',
        'header_system': 'родро╛ройро┐ропроЩрпНроХро┐ роЖродро╛ро░ роЪрпЖропро▓ро╛роХрпНроХ роЕроорпИрокрпНрокрпБ',
        'header_police': 'родрооро┐ро┤рпНроиро╛роЯрпБ роХро╛ро╡ро▓рпН - роЪро╛ройрпНро▒ро│ро┐роХрпНроХрокрпНрокроЯрпНроЯ рокроХрпБрокрпНрокро╛ропрпНро╡рпБ',
        'classification': 'роХроЯрпНроЯрпБрокрпНрокроЯрпБродрпНродрокрпНрокроЯрпНроЯ',
        'case_info': '1. ро╡ро┤роХрпНроХрпБ родроХро╡ро▓рпН',
        'case_number': 'ро╡ро┤роХрпНроХрпБ роОрогрпН:',
        'investigating_officer': 'ро╡ро┐роЪро╛ро░рогрпИ роЕродро┐роХро╛ро░ро┐:',
        'station': 'роХро╛ро╡ро▓рпН роиро┐ро▓рпИропроорпН:',
        'location_incident': 'роЪроорпНрокро╡ роЗроЯроорпН:',
        'date_incident': 'роЪроорпНрокро╡ родрпЗродро┐:',
        'time_incident': 'роЪроорпНрокро╡ роирпЗро░роорпН:',
        'executive_summary': '2. роЪрпЖропро▓рпНродро┐ро▒ройрпН роЪрпБро░рпБроХрпНроХроорпН',
        'detailed_analysis': '3. ро╡ро┐ро░ро┐ро╡ро╛рой роЖродро╛ро░ рокроХрпБрокрпНрокро╛ропрпНро╡рпБ',
        'chronological_timeline': '4. роХро╛ро▓ро╡ро░ро┐роЪрпИ роиро┐роХро┤рпНро╡рпБроХро│рпН',
        'key_evidence_findings': '5. роорпБроХрпНроХро┐роп роЖродро╛ро░ роХрогрпНроЯрпБрокро┐роЯро┐рокрпНрокрпБроХро│рпН',
        'critical_identifiers': '6. роорпБроХрпНроХро┐роп роЕроЯрпИропро╛ро│роЩрпНроХро╛роЯрпНроЯро┐роХро│рпН',
        'investigative_recommendations': '7. ро╡ро┐роЪро╛ро░рогрпИ рокро░ро┐роирпНродрпБро░рпИроХро│рпН',
        'analytical_intelligence': '8. рокроХрпБрокрпНрокро╛ропрпНро╡рпБ роирпБрогрпНрогро▒ро┐ро╡рпБ',
        'visual_evidence': '9. роХро╛роЯрпНроЪро┐ роЖродро╛ро░ рокроЯроЩрпНроХро│рпН',
        'confidence_metrics': '10. роироорпНрокроХродрпНродройрпНроорпИ роЕро│ро╡рпАроЯрпБроХро│рпН',
        'report_generated_by': 'роЕро▒ро┐роХрпНроХрпИ роЙро░рпБро╡ро╛роХрпНроХро┐ропродрпБ:',
        'system_version': 'роЕроорпИрокрпНрокрпБ рокродро┐рокрпНрокрпБ:',
        'analysis_confidence': 'рокроХрпБрокрпНрокро╛ропрпНро╡рпБ роироорпНрокроХродрпНродройрпНроорпИ:',
        'computer_generated_note': 'роЗроирпНрод роЕро▒ро┐роХрпНроХрпИ роХрогро┐ройро┐ роЙро░рпБро╡ро╛роХрпНроХрокрпНрокроЯрпНроЯродрпБ рооро▒рпНро▒рпБроорпН ро╡ро┐роЪро╛ро░рогрпИ роЕродро┐роХро╛ро░ро┐роХро│ро╛ро▓рпН роЪро░ро┐рокро╛ро░рпНроХрпНроХрокрпНрокроЯ ро╡рпЗрогрпНроЯрпБроорпН.',
        'investigating_officer_signature': 'ро╡ро┐роЪро╛ро░рогрпИ роЕродро┐роХро╛ро░ро┐ роХрпИропрпКрокрпНрокроорпН:',
        'reviewing_officer_signature': 'роородро┐рокрпНрокро╛ропрпНро╡рпБ роЕродро┐роХро╛ро░ро┐ роХрпИропрпКрокрпНрокроорпН:',
        'date': 'родрпЗродро┐:',
        'footer_text': 'роЕро▓рпНрокро╛ ро▓рпЗрокрпНро╕рпН роЖродро╛ро░ рокроХрпБрокрпНрокро╛ропрпНро╡рпБ роЕроорпИрокрпНрокрпБ - роЗро░роХроЪро┐ропроорпН',
        'header_text': 'родро╛ройро┐ропроЩрпНроХро┐ роЖродро╛ро░ рокроХрпБрокрпНрокро╛ропрпНро╡рпБ роЕро▒ро┐роХрпНроХрпИ',
        'no_summary': 'роЪрпЖропро▓рпНродро┐ро▒ройрпН роЪрпБро░рпБроХрпНроХроорпН роЗро▓рпНро▓рпИ.',
        'no_analysis': 'ро╡ро┐ро░ро┐ро╡ро╛рой рокроХрпБрокрпНрокро╛ропрпНро╡рпБ роЗро▓рпНро▓рпИ.',
        'no_timeline': 'рокроХрпБрокрпНрокро╛ропрпНро╡ро┐ро▓ро┐ро░рпБроирпНродрпБ ро╡рпЖро│ро┐рокрпНрокроЯрпИропро╛рой роХро╛ро▓ро╡ро░ро┐роЪрпИропрпИ рокро┐ро░ро┐родрпНродрпЖроЯрпБроХрпНроХ роорпБроЯро┐ропро╡ро┐ро▓рпНро▓рпИ.',
        'no_findings': 'роХрпБро▒ро┐рокрпНрокро┐роЯрпНроЯ роорпБроХрпНроХро┐роп роХрогрпНроЯрпБрокро┐роЯро┐рокрпНрокрпБроХро│рпИ родро╛ройро╛роХ рокро┐ро░ро┐родрпНродрпЖроЯрпБроХрпНроХ роорпБроЯро┐ропро╡ро┐ро▓рпНро▓рпИ.',
        'no_identifiers': 'роорпБроХрпНроХро┐роп роЕроЯрпИропро╛ро│роЩрпНроХро╛роЯрпНроЯро┐роХро│рпН (ро╡ро╛роХрой родроЯрпНроЯрпБроХро│рпН, родрпКро▓рпИрокрпЗроЪро┐ роОрогрпНроХро│рпН) родро╛ройро╛роХ роХрогрпНроЯро▒ро┐ропрокрпНрокроЯро╡ро┐ро▓рпНро▓рпИ.',
        'vehicle_plates': 'ро╡ро╛роХрой родроЯрпНроЯрпБроХро│рпН:',
        'phone_numbers': 'родрпКро▓рпИрокрпЗроЪро┐ роОрогрпНроХро│рпН:',
        'event': 'роиро┐роХро┤рпНро╡рпБ',
        'finding': 'роХрогрпНроЯрпБрокро┐роЯро┐рокрпНрокрпБ',
        'analysis_point': 'рокроХрпБрокрпНрокро╛ропрпНро╡рпБ рокрпБро│рпНро│ро┐',
        'risk_level': 'роЖрокродрпНродрпБ роиро┐ро▓рпИ:',
        'confidence_score': 'роироорпНрокроХродрпНродройрпНроорпИ роородро┐рокрпНрокрпЖрогрпН:',
        'analytical_insights': 'рокроХрпБрокрпНрокро╛ропрпНро╡рпБ роирпБрогрпНрогро▒ро┐ро╡рпБроХро│рпН:',
        'key_frames': 'роорпБроХрпНроХро┐роп роХро╛роЯрпНроЪро┐ роЖродро╛ро░роЩрпНроХро│рпН:',
        'risk_assessment': 'роЖрокродрпНродрпБ роородро┐рокрпНрокрпАроЯрпБ',
        'threat_level': 'роЕроЪрпНроЪрпБро▒рпБродрпНродро▓рпН роиро┐ро▓рпИ',
        'recommended_response': 'рокро░ро┐роирпНродрпБро░рпИроХрпНроХрокрпНрокроЯрпНроЯ рокродро┐ро▓рпН'
    },
    'kn': {
        'title': 'р▓Жр▓▓р│Нр▓лр▓╛ р▓▓р│Нр▓пр▓╛р▓мр│Нр▓╕р│Н - р▓кр│Бр▓░р▓╛р▓╡р│Ж р▓╡р▓┐р▓╢р│Нр▓▓р│Зр▓╖р▓гр▓╛ р▓╡р▓░р▓жр▓┐',
        'header_system': 'р▓╕р│Нр▓╡р▓пр▓Вр▓Ър▓╛р▓▓р▓┐р▓д р▓кр│Бр▓░р▓╛р▓╡р│Ж р▓╕р▓Вр▓╕р│Нр▓Хр▓░р▓гр▓╛ р▓╡р│Нр▓пр▓╡р▓╕р│Нр▓ер│Ж',
        'header_police': 'р▓др▓ор▓┐р▓│р│Бр▓ир▓╛р▓бр│Б р▓кр│Кр▓▓р│Ар▓╕р│Н - р▓кр│Нр▓░р▓ор▓╛р▓гр│Ар▓Хр│Гр▓д р▓╡р▓┐р▓╢р│Нр▓▓р│Зр▓╖р▓гр│Ж',
        'classification': 'р▓ир▓┐р▓░р│Нр▓мр▓Вр▓зр▓┐р▓д',
        'case_info': '1. р▓кр│Нр▓░р▓Хр▓░р▓гр▓ж р▓ор▓╛р▓╣р▓┐р▓др▓┐',
        'case_number': 'р▓кр│Нр▓░р▓Хр▓░р▓г р▓╕р▓Вр▓Цр│Нр▓пр│Ж:',
        'investigating_officer': 'р▓╡р▓┐р▓Ър▓╛р▓░р▓гр▓╛ р▓Ер▓зр▓┐р▓Хр▓╛р▓░р▓┐:',
        'station': 'р▓кр│Кр▓▓р│Ар▓╕р│Н р▓ир▓┐р▓▓р▓п:',
        'location_incident': 'р▓Шр▓Яр▓ир│Жр▓п р▓╕р│Нр▓ер▓│:',
        'date_incident': 'р▓Шр▓Яр▓ир│Жр▓п р▓жр▓┐р▓ир▓╛р▓Вр▓Х:',
        'time_incident': 'р▓Шр▓Яр▓ир│Жр▓п р▓╕р▓ор▓п:',
        'executive_summary': '2. р▓Хр▓╛р▓░р│Нр▓пр▓ир▓┐р▓░р│Нр▓╡р▓╛р▓╣р▓Х р▓╕р▓╛р▓░р▓╛р▓Вр▓╢',
        'detailed_analysis': '3. р▓╡р▓┐р▓╡р▓░р▓╡р▓╛р▓ж р▓кр│Бр▓░р▓╛р▓╡р│Ж р▓╡р▓┐р▓╢р│Нр▓▓р│Зр▓╖р▓гр│Ж',
        'chronological_timeline': '4. р▓Хр▓╛р▓▓р▓Хр│Нр▓░р▓ор▓╛р▓ир│Бр▓Чр▓д р▓Яр│Ир▓ор│Нр▓▓р│Ир▓ир│Н',
        'key_evidence_findings': '5. р▓кр│Нр▓░р▓ор│Бр▓Ц р▓кр│Бр▓░р▓╛р▓╡р│Ж р▓Хр▓Вр▓бр│Бр▓╣р▓┐р▓бр▓┐р▓жр▓▓р│Б',
        'critical_identifiers': '6. р▓ир▓┐р▓░р│Нр▓гр▓╛р▓пр▓Х р▓Чр│Бр▓░р│Бр▓др▓┐р▓╕р│Бр▓╡р▓┐р▓Хр│Жр▓Чр▓│р│Б',
        'investigative_recommendations': '7. р▓╡р▓┐р▓Ър▓╛р▓░р▓гр▓╛ р▓╢р▓┐р▓лр▓╛р▓░р▓╕р│Бр▓Чр▓│р│Б',
        'analytical_intelligence': '8. р▓╡р▓┐р▓╢р│Нр▓▓р│Зр▓╖р▓гр▓╛р▓др│Нр▓ор▓Х р▓мр│Бр▓жр│Нр▓зр▓┐р▓ор▓др│Нр▓др│Ж',
        'visual_evidence': '9. р▓жр│Гр▓╢р│Нр▓п р▓кр│Бр▓░р▓╛р▓╡р│Ж р▓╕р│Нр▓ир│Нр▓пр▓╛р▓кр│Нр▓╢р▓╛р▓Яр│Нр▓Чр▓│р│Б',
        'confidence_metrics': '10. р▓╡р▓┐р▓╢р│Нр▓╡р▓╛р▓╕р▓╛р▓░р│Нр▓╣р▓др│Ж р▓ор│Жр▓Яр│Нр▓░р▓┐р▓Хр│Нр▓╕р│Н',
        'report_generated_by': 'р▓╡р▓░р▓жр▓┐ р▓░р▓Ър▓┐р▓╕р▓┐р▓жр▓╡р▓░р│Б:',
        'system_version': 'р▓╕р▓┐р▓╕р│Нр▓Яр▓ор│Н р▓Жр▓╡р│Гр▓др│Нр▓др▓┐:',
        'analysis_confidence': 'р▓╡р▓┐р▓╢р│Нр▓▓р│Зр▓╖р▓гр▓╛ р▓╡р▓┐р▓╢р│Нр▓╡р▓╛р▓╕р▓╛р▓░р│Нр▓╣р▓др│Ж:',
        'computer_generated_note': 'р▓И р▓╡р▓░р▓жр▓┐р▓пр│Б р▓Хр▓Вр▓кр│Нр▓пр│Вр▓Яр▓░р│Н р▓░р▓Ър▓┐р▓др▓╡р▓╛р▓Чр▓┐р▓жр│Ж р▓ор▓др│Нр▓др│Б р▓╡р▓┐р▓Ър▓╛р▓░р▓гр▓╛ р▓Ер▓зр▓┐р▓Хр▓╛р▓░р▓┐р▓Чр▓│р▓┐р▓Вр▓ж р▓кр▓░р▓┐р▓╢р│Ар▓▓р▓┐р▓╕р▓мр│Зр▓Хр│Б.',
        'investigating_officer_signature': 'р▓╡р▓┐р▓Ър▓╛р▓░р▓гр▓╛ р▓Ер▓зр▓┐р▓Хр▓╛р▓░р▓┐ р▓╕р▓╣р▓┐:',
        'reviewing_officer_signature': 'р▓кр▓░р▓┐р▓╢р│Ар▓▓р▓ир▓╛ р▓Ер▓зр▓┐р▓Хр▓╛р▓░р▓┐ р▓╕р▓╣сГШ:',
        'date': 'р▓жр▓┐р▓ир▓╛р▓Вр▓Х:',
        'footer_text': 'р▓Жр▓▓р│Нр▓лр▓╛ р▓▓р│Нр▓пр▓╛р▓мр│Нр▓╕р│Н р▓кр│Бр▓░р▓╛р▓╡р│Ж р▓╡р▓┐р▓╢р│Нр▓▓р│Зр▓╖р▓гр▓╛ р▓╡р│Нр▓пр▓╡р▓╕р│Нр▓ер│Ж - р▓Чр│Мр▓кр│Нр▓п',
        'header_text': 'р▓╕р│Нр▓╡р▓пр▓Вр▓Ър▓╛р▓▓р▓┐р▓д р▓кр│Бр▓░р▓╛р▓╡р│Ж р▓╡р▓┐р▓╢р│Нр▓▓р│Зр▓╖р▓гр▓╛ р▓╡р▓░р▓жр▓┐',
        'no_summary': 'р▓Хр▓╛р▓░р│Нр▓пр▓ир▓┐р▓░р│Нр▓╡р▓╛р▓╣р▓Х р▓╕р▓╛р▓░р▓╛р▓Вр▓╢ р▓▓р▓нр│Нр▓пр▓╡р▓┐р▓▓р│Нр▓▓.',
        'no_analysis': 'р▓╡р▓┐р▓╡р▓░р▓╡р▓╛р▓ж р▓╡р▓┐р▓╢р│Нр▓▓р│Зр▓╖р▓гр│Ж р▓▓р▓нр│Нр▓пр▓╡р▓┐р▓▓р│Нр▓▓.',
        'no_timeline': 'р▓╡р▓┐р▓╢р│Нр▓▓р│Зр▓╖р▓гр│Жр▓пр▓┐р▓Вр▓ж р▓╕р│Нр▓кр▓╖р│Нр▓Я р▓Яр│Ир▓ор│Нр▓▓р│Ир▓ир│Н р▓Ер▓ир│Нр▓ир│Б р▓╣р│Кр▓░р▓др│Жр▓Чр│Жр▓пр▓▓р│Б р▓╕р▓╛р▓зр│Нр▓пр▓╡р▓╛р▓Чр▓▓р▓┐р▓▓р│Нр▓▓.',
        'no_findings': 'р▓ир▓┐р▓░р│Нр▓жр▓┐р▓╖р│Нр▓Я р▓кр│Нр▓░р▓ор│Бр▓Ц р▓Хр▓Вр▓бр│Бр▓╣р▓┐р▓бр▓┐р▓жр▓▓р│Б р▓╕р│Нр▓╡р▓пр▓Вр▓Ър▓╛р▓▓р▓┐р▓др▓╡р▓╛р▓Чр▓┐ р▓╣р│Кр▓░р▓др│Жр▓Чр│Жр▓пр▓▓р│Б р▓╕р▓╛р▓зр│Нр▓пр▓╡р▓╛р▓Чр▓▓р▓┐р▓▓р│Нр▓▓.',
        'no_identifiers': 'р▓пр▓╛р▓╡р│Бр▓жр│З р▓ир▓┐р▓░р│Нр▓гр▓╛р▓пр▓Х р▓Чр│Бр▓░р│Бр▓др▓┐р▓╕р│Бр▓╡р▓┐р▓Хр│Жр▓Чр▓│р│Б (р▓╡р▓╛р▓╣р▓и р▓кр│Нр▓▓р│Зр▓Яр│Нр▓Чр▓│р│Б, р▓лр│Лр▓ир│Н р▓╕р▓Вр▓Цр│Нр▓пр│Жр▓Чр▓│р│Б) р▓╕р│Нр▓╡р▓пр▓Вр▓Ър▓╛р▓▓р▓┐р▓др▓╡р▓╛р▓Чр▓┐ р▓кр▓др│Нр▓др│Жр▓пр▓╛р▓Чр▓┐р▓▓р│Нр▓▓.',
        'vehicle_plates': 'р▓╡р▓╛р▓╣р▓и р▓кр│Нр▓▓р│Зр▓Яр│Нр▓Чр▓│р│Б:',
        'phone_numbers': 'р▓лр│Лр▓ир│Н р▓╕р▓Вр▓Цр│Нр▓пр│Жр▓Чр▓│р│Б:',
        'event': 'р▓Шр▓Яр▓ир│Ж',
        'finding': 'р▓Хр▓Вр▓бр│Бр▓╣р▓┐р▓бр▓┐р▓жр▓▓р│Б',
        'analysis_point': 'р▓╡р▓┐р▓╢р│Нр▓▓р│Зр▓╖р▓гр▓╛ р▓мр▓┐р▓Вр▓жр│Б',
        'risk_level': 'р▓Ер▓кр▓╛р▓пр▓ж р▓ор▓Яр│Нр▓Я:',
        'confidence_score': 'р▓╡р▓┐р▓╢р│Нр▓╡р▓╛р▓╕р▓╛р▓░р│Нр▓╣р▓др│Ж р▓╕р│Нр▓Хр│Лр▓░р│Н:',
        'analytical_insights': 'р▓╡р▓┐р▓╢р│Нр▓▓р│Зр▓╖р▓гр▓╛р▓др│Нр▓ор▓Х р▓Тр▓│р▓ир│Лр▓Яр▓Чр▓│р│Б:',
        'key_frames': 'р▓кр│Нр▓░р▓ор│Бр▓Ц р▓жр│Гр▓╢р│Нр▓п р▓кр│Бр▓░р▓╛р▓╡р│Жр▓Чр▓│р│Б:',
        'risk_assessment': 'р▓Ер▓кр▓╛р▓п р▓ор│Мр▓▓р│Нр▓пр▓ор▓╛р▓кр▓и',
        'threat_level': 'р▓мр│Жр▓жр▓░р▓┐р▓Хр│Жр▓п р▓ор▓Яр│Нр▓Я',
        'recommended_response': 'р▓╢р▓┐р▓лр▓╛р▓░р▓╕р│Б р▓ор▓╛р▓бр▓▓р▓╛р▓ж р▓кр│Нр▓░р▓др▓┐р▓Хр│Нр▓░р▓┐р▓пр│Ж'
    }
}

# Recommendations in different languages
RECOMMENDATIONS = {
    'en': [
        "Review extracted timeline for sequence verification",
        "Validate all identified vehicle registration numbers",
        "Correlate timestamps with other evidence sources",
        "Verify locations mentioned in the analysis",
        "Follow up on identified phone numbers if relevant",
        "Conduct field verification of key observations",
        "Cross-reference with other case evidence"
    ],
    'ta': [
        "ро╡ро░ро┐роЪрпИ роЪро░ро┐рокро╛ро░рпНрокрпНрокро┐ро▒рпНроХро╛роХ рокро┐ро░ро┐родрпНродрпЖроЯрпБроХрпНроХрокрпНрокроЯрпНроЯ роХро╛ро▓ро╡ро░ро┐роЪрпИропрпИ роородро┐рокрпНрокро╛ропрпНро╡рпБ роЪрпЖропрпНропро╡рпБроорпН",
        "роЕроЯрпИропро╛ро│роорпН роХро╛рогрокрпНрокроЯрпНроЯ роЕройрпИродрпНродрпБ ро╡ро╛роХрой рокродро┐ро╡рпБ роОрогрпНроХро│рпИропрпБроорпН роЪро░ро┐рокро╛ро░рпНроХрпНроХро╡рпБроорпН",
        "рооро▒рпНро▒ роЖродро╛ро░ роЖродро╛ро░роЩрпНроХро│рпБроЯройрпН роирпЗро░ роорпБродрпНродро┐ро░рпИроХро│рпИ родрпКроЯро░рпНрокрпБрокроЯрпБродрпНродро╡рпБроорпН",
        "рокроХрпБрокрпНрокро╛ропрпНро╡ро┐ро▓рпН роХрпБро▒ро┐рокрпНрокро┐роЯрокрпНрокроЯрпНроЯ роЗроЯроЩрпНроХро│рпИ роЪро░ро┐рокро╛ро░рпНроХрпНроХро╡рпБроорпН",
        "родрпКроЯро░рпНрокрпБроЯрпИропродро╛роХ роЗро░рпБроирпНродро╛ро▓рпН роЕроЯрпИропро╛ро│роорпН роХро╛рогрокрпНрокроЯрпНроЯ родрпКро▓рпИрокрпЗроЪро┐ роОрогрпНроХро│рпИ рокро┐ройрпНродрпКроЯро░ро╡рпБроорпН",
        "роорпБроХрпНроХро┐роп роХрогрпНроХро╛рогро┐рокрпНрокрпБроХро│ро┐ройрпН роХро│ роЪро░ро┐рокро╛ро░рпНрокрпНрокрпИ роорпЗро▒рпНроХрпКро│рпНро│ро╡рпБроорпН",
        "рооро▒рпНро▒ ро╡ро┤роХрпНроХрпБ роЖродро╛ро░роЩрпНроХро│рпБроЯройрпН роХрпБро▒рпБроХрпНроХрпБ роХрпБро▒ро┐рокрпНрокрпБ роЪрпЖропрпНропро╡рпБроорпН"
    ],
    'kn': [
        "р▓Ер▓ир│Бр▓Хр│Нр▓░р▓о р▓кр▓░р▓┐р▓╢р│Ар▓▓р▓ир│Жр▓Чр▓╛р▓Чр▓┐ р▓╣р│Кр▓░р▓др│Жр▓Чр│Жр▓ж р▓Яр│Ир▓ор│Нр▓▓р│Ир▓ир│Н р▓Ер▓ир│Нр▓ир│Б р▓кр▓░р▓┐р▓╢р│Ар▓▓р▓┐р▓╕р▓┐",
        "р▓Чр│Бр▓░р│Бр▓др▓┐р▓╕р▓▓р▓╛р▓ж р▓Ор▓▓р│Нр▓▓р▓╛ р▓╡р▓╛р▓╣р▓и р▓ир│Лр▓Вр▓жр▓гр▓┐ р▓╕р▓Вр▓Цр│Нр▓пр│Жр▓Чр▓│р▓ир│Нр▓ир│Б р▓ор│Мр▓▓р│Нр▓пр│Ар▓Хр▓░р▓┐р▓╕р▓┐",
        "р▓Зр▓др▓░ р▓кр│Бр▓░р▓╛р▓╡р│Ж р▓ор│Вр▓▓р▓Чр▓│р│Кр▓Вр▓жр▓┐р▓Чр│Ж р▓Яр│Ир▓ор│Нр▓╕р│Нр▓Яр▓╛р▓Вр▓кр│Нр▓Чр▓│р▓ир│Нр▓ир│Б р▓╕р▓Вр▓мр▓Вр▓зр▓кр▓бр▓┐р▓╕р▓┐",
        "р▓╡р▓┐р▓╢р│Нр▓▓р│Зр▓╖р▓гр│Жр▓пр▓▓р│Нр▓▓р▓┐ р▓Йр▓▓р│Нр▓▓р│Зр▓Цр▓┐р▓╕р▓▓р▓╛р▓ж р▓╕р│Нр▓ер▓│р▓Чр▓│р▓ир│Нр▓ир│Б р▓кр▓░р▓┐р▓╢р│Ар▓▓р▓┐р▓╕р▓┐",
        "р▓╕р▓Вр▓мр▓Вр▓зр▓┐р▓др▓╡р▓╛р▓Чр▓┐р▓жр│Нр▓жр▓░р│Ж р▓Чр│Бр▓░р│Бр▓др▓┐р▓╕р▓▓р▓╛р▓ж р▓лр│Лр▓ир│Н р▓╕р▓Вр▓Цр│Нр▓пр│Жр▓Чр▓│р▓ир│Нр▓ир│Б р▓ор│Бр▓Вр▓жр│Бр▓╡р▓░р▓┐р▓╕р▓┐",
        "р▓кр│Нр▓░р▓ор│Бр▓Ц р▓╡р│Ар▓Хр│Нр▓╖р▓гр│Жр▓Чр▓│ р▓Хр│Нр▓╖р│Зр▓др│Нр▓░ р▓кр▓░р▓┐р▓╢р│Ар▓▓р▓ир│Жр▓пр▓ир│Нр▓ир│Б р▓ир▓бр│Жр▓╕р▓┐",
        "р▓Зр▓др▓░ р▓кр│Нр▓░р▓Хр▓░р▓г р▓кр│Бр▓░р▓╛р▓╡р│Жр▓Чр▓│р│Кр▓Вр▓жр▓┐р▓Чр│Ж р▓Хр│Нр▓░р▓╛р▓╕р│Н-р▓░р│Жр▓лр▓░р│Жр▓ир│Нр▓╕р│Н р▓ор▓╛р▓бр▓┐"
    ]
}

# ========== MISSING FUNCTIONS ADDED HERE ==========

def _split_into_readable_paragraphs(text, max_sentences=3):
    """Split text into readable paragraphs with sentence limits"""
    if not text:
        return []
        
    paragraphs = []
    current_para = []
    
    # Split into sentences
    sentences = re.split(r'(?<=[.!?])\s+', text)
    
    for sentence in sentences:
        sentence = sentence.strip()
        if sentence and len(sentence) > 10:  # Filter very short sentences
            current_para.append(sentence)
            
            # Start new paragraph after max sentences
            if len(current_para) >= max_sentences:
                paragraphs.append(' '.join(current_para))
                current_para = []
    
    # Add remaining sentences
    if current_para:
        paragraphs.append(' '.join(current_para))
    
    return paragraphs if paragraphs else [text]

def _extract_chronology(text):
    """Enhanced chronology extraction with better time parsing"""
    if not text:
        return []
        
    lines = [s.strip() for s in re.split(r'[\n]+', text) if s.strip()]
    time_pattern = re.compile(r'\b([01]?\d|2[0-3]):[0-5]\d(?::[0-5]\d)?\b')
    
    chrono_entries = []
    for line in lines:
        if time_pattern.search(line):
            # Extract time and create structured entry
            times = time_pattern.findall(line)
            if times:
                # Clean up the description
                description = line.strip()
                # Remove the time from beginning if it's there for cleaner display
                if description.startswith(times[0]):
                    description = description[len(times[0]):].strip(' -:')
                
                chrono_entries.append({
                    'time': times[0],
                    'description': description,
                    'sort_key': _time_to_seconds(times[0])
                })
    
    if chrono_entries:
        # Sort by time and return formatted entries
        chrono_entries.sort(key=lambda x: x['sort_key'])
        return [f"{entry['time']} - {entry['description']}" for entry in chrono_entries[:12]]
    
    # Fallback: return meaningful lines that might contain event descriptions
    event_keywords = ['first', 'then', 'after', 'before', 'begins', 'starts', 'ends', 'appears', 'enters', 'exits']
    event_lines = [line for line in lines if any(keyword in line.lower() for keyword in event_keywords) and len(line) > 15]
    return event_lines[:8]

def _time_to_seconds(time_str):
    """Convert time string to seconds for sorting"""
    try:
        parts = list(map(int, time_str.split(':')))
        while len(parts) < 3:
            parts.append(0)
        return parts[0] * 3600 + parts[1] * 60 + parts[2]
    except:
        return 0

def _extract_key_observations(text):
    """Enhanced key observations extraction"""
    if not text:
        return []
        
    lines = [s.strip() for s in re.split(r'[\n]+', text) if s.strip() and len(s.strip()) > 20]
    
    # Priority indicators for key observations
    priority_indicators = [
        (r'\b(weapon|gun|knife|firearm|explosive)\b', 5),
        (r'\b(vehicle|car|truck|motorcycle|scooter)\b', 4),
        (r'\b(suspect|person|individual|man|woman)\b', 3),
        (r'\b(TN[-\s]?\d{1,2}[-\s]?[A-Z]{1,2}[-\s]?\d{1,4})\b', 5),
        (r'\b(money|cash|amount|rupee|currency)\b', 4),
        (r'\b(drug|narcotic|substance|illegal)\b', 5),
        (r'\b(fight|fight|attack|assault|violence)\b', 5),
        (r'\b(punch|hit|kick|grapple|strike)\b', 4),
    ]
    
    scored_lines = []
    for line in lines:
        score = 0
        for pattern, points in priority_indicators:
            if re.search(pattern, line, re.IGNORECASE):
                score += points
        if score > 0:
            scored_lines.append((line, score))
    
    # Sort by score and return top observations
    scored_lines.sort(key=lambda x: x[1], reverse=True)
    return [line for line, score in scored_lines[:10]]

def _highlight_evidences(text):
    """Enhanced evidence highlighting with better color scheme"""
    if not text:
        return ""
        
    # Escape HTML characters
    text = text.replace('<', '&lt;').replace('>', '&gt;')
    
    # Patterns for different types of evidence
    time_pattern = re.compile(r'\b([01]?\d|2[0-3]):[0-5]\d(?::[0-5]\d)?\b')
    plate_pattern = re.compile(r'\b(TN[-\s]?\d{1,2}[-\s]?[A-Z]{1,2}[-\s]?\d{1,4})\b', re.IGNORECASE)
    phone_pattern = re.compile(r'\b(\d{3}[-.\s]??\d{3}[-.\s]??\d{4}|\(\d{3}\)\s*\d{3}[-.\s]??\d{4}|\d{3}[-.\s]??\d{4})\b')
    
    places = ["Chennai", "Coimbatore", "Madurai", "Tiruchirappalli", "Tirunelveli", 
              "Salem", "Tiruppur", "Erode", "Kanchipuram", "Vellore"]
    
    # Priority-based highlighting (order matters)
    
    # 1. Vehicle plates (Highest priority - Red)
    text = plate_pattern.sub(
        lambda m: f'<font color="#D32F2F"><b>ЁЯЪЧ {m.group(0)}</b></font>', text)
    
    # 2. Timestamps (Blue)
    text = time_pattern.sub(
        lambda m: f'<font color="#1976D2"><b>тП░ {m.group(0)}</b></font>', text)
    
    # 3. Phone numbers (Green)
    text = phone_pattern.sub(
        lambda m: f'<font color="#388E3C"><b>ЁЯУЮ {m.group(0)}</b></font>', text)
    
    # 4. Locations (Purple)
    for place in places:
        text = re.sub(r'\b' + re.escape(place) + r'\b', 
                     f'<font color="#7B1FA2"><b>ЁЯУН {place}</b></font>', text, flags=re.IGNORECASE)
    
    # 5. Evidence keywords (Orange)
    evidence_terms = ['evidence', 'suspicious', 'weapon', 'suspect', 'crime', 'illegal', 'theft', 'accident']
    for term in evidence_terms:
        text = re.sub(r'\b' + re.escape(term) + r'\b', 
                     f'<font color="#F57C00"><b>{term.upper()}</b></font>', text, flags=re.IGNORECASE)
    
    return text

def _extract_ai_sections(analysis_text):
    """Extract and structure AI analysis into sections with better parsing"""
    if not analysis_text:
        return {}
    
    sections = {
        'executive_summary': '',
        'detailed_analysis': '',
        'key_findings': '',
        'chronology': ''
    }
    
    # Clean and normalize text
    lines = [line.strip() for line in analysis_text.split('\n') if line.strip()]
    
    # Improved section detection with multiple patterns
    current_section = 'executive_summary'  # Default to executive summary
    section_content = []
    
    for i, line in enumerate(lines):
        lower_line = line.lower()
        
        # Detect section headers with multiple patterns
        if any(keyword in lower_line for keyword in ['executive summary', 'summary', 'overview']):
            if section_content:
                sections[current_section] = ' '.join(section_content)
            section_content = []
            current_section = 'executive_summary'
            
        elif any(keyword in lower_line for keyword in ['detailed analysis', 'analysis', 'detailed evidence']):
            if section_content:
                sections[current_section] = ' '.join(section_content)
            section_content = []
            current_section = 'detailed_analysis'
            
        elif any(keyword in lower_line for keyword in ['key findings', 'key evidence', 'findings', 'observations']):
            if section_content:
                sections[current_section] = ' '.join(section_content)
            section_content = []
            current_section = 'key_findings'
            
        elif any(keyword in lower_line for keyword in ['timeline', 'chronology', 'chronological', 'sequence of events']):
            if section_content:
                sections[current_section] = ' '.join(section_content)
            section_content = []
            current_section = 'chronology'
            
        elif re.search(r'\b\d{1,2}:\d{2}', line):  # Time pattern
            # If we find timestamps and we're not already in chronology, switch
            if current_section != 'chronology' and len(section_content) > 3:
                if section_content:
                    sections[current_section] = ' '.join(section_content)
                section_content = [line]  # Start with this timestamp line
                current_section = 'chronology'
            else:
                section_content.append(line)
        else:
            section_content.append(line)
    
    # Add the last section
    if section_content:
        sections[current_section] = ' '.join(section_content)
    
    # Fallback: If no good sections found, create intelligent splits
    if not any(len(str(section).strip()) > 50 for section in sections.values()):
        return _create_intelligent_sections(analysis_text)
    
    return sections

def _create_intelligent_sections(full_text):
    """Create sections intelligently when automatic parsing fails"""
    paragraphs = [p.strip() for p in full_text.split('\n') if p.strip() and len(p.strip()) > 20]
    
    sections = {
        'executive_summary': '',
        'detailed_analysis': '',
        'key_findings': '',
        'chronology': ''
    }
    
    if not paragraphs:
        return sections
    
    # Executive summary = first 1-2 paragraphs
    sections['executive_summary'] = ' '.join(paragraphs[:2])
    
    # Detailed analysis = most content
    if len(paragraphs) > 2:
        sections['detailed_analysis'] = ' '.join(paragraphs[2:])
    
    # Extract chronology from lines with timestamps
    time_pattern = re.compile(r'\b([01]?\d|2[0-3]):[0-5]\d(?::[0-5]\d)?\b')
    time_lines = [line for line in full_text.split('\n') if time_pattern.search(line)]
    if time_lines:
        sections['chronology'] = ' '.join(time_lines[:10])  # Limit to 10 time entries
    
    # Key findings = lines with evidence keywords
    evidence_keywords = ['vehicle', 'plate', 'TN-', 'suspect', 'weapon', 'suspicious', 'individual', 'person', 'fight', 'altercation']
    key_lines = []
    for line in full_text.split('\n'):
        if any(keyword in line.lower() for keyword in evidence_keywords) and len(line) > 20:
            key_lines.append(line)
    if key_lines:
        sections['key_findings'] = ' '.join(key_lines[:8])  # Limit to 8 key findings
    
    return sections

# ========== END OF MISSING FUNCTIONS ==========

def _register_fonts(language='en'):
    """Register fonts with Unicode support for different languages"""
    try:
        # Use built-in fonts instead of external files
        # These are available in most systems
        builtin_fonts = {
            'en': 'Helvetica',
            'ta': 'Helvetica',  # Fallback to Helvetica for Tamil
            'kn': 'Helvetica'   # Fallback to Helvetica for Kannada
        }
        
        return builtin_fonts.get(language, 'Helvetica')
            
    except Exception as e:
        print(f"Font registration warning: {e}")
        return "Helvetica"

def _create_watermark(canvas, doc, language='en'):
    """Create professional watermark on every page"""
    canvas.saveState()
    
    # Set watermark properties
    canvas.setFont('Helvetica-Bold', 42)
    canvas.setFillColor(colors.HexColor('#F0F0F0'))
    canvas.setFillAlpha(0.1)
    
    # Get page dimensions
    page_width = doc.pagesize[0]
    page_height = doc.pagesize[1]
    
    # Rotate and position watermark in center
    canvas.translate(page_width / 2, page_height / 2)
    canvas.rotate(45)
    
    # Draw main watermark text
    watermark_texts = {
        'en': "ALFA LABS",
        'ta': "роЕро▓рпНрокро╛ ро▓рпЗрокрпНро╕рпН",
        'kn': "р▓Жр▓▓р│Нр▓лр▓╛ р▓▓р│Нр▓пр▓╛р▓мр│Нр▓╕р│Н"
    }
    canvas.drawCentredString(0, 0, watermark_texts.get(language, "ALFA LABS"))
    
    canvas.restoreState()
    
    # Add header and footer on every page
    canvas.saveState()
    
    # Header line
    canvas.setStrokeColor(colors.HexColor('#2C5530'))
    canvas.setLineWidth(0.5)
    canvas.line(doc.leftMargin, doc.height + doc.topMargin - 15, 
                doc.width + doc.leftMargin, doc.height + doc.topMargin - 15)
    
    # Footer line
    canvas.line(doc.leftMargin, doc.bottomMargin + 15, 
                doc.width + doc.leftMargin, doc.bottomMargin + 15)
    
    # Page number
    canvas.setFont('Helvetica', 8)
    canvas.setFillColor(colors.gray)
    page_text = f"Page {canvas.getPageNumber()}"
    canvas.drawRightString(doc.width + doc.leftMargin - 10, doc.bottomMargin, page_text)
    
    # Footer text
    footer_texts = {
        'en': "Alfa Labs Evidence Analysis System - Confidential",
        'ta': "роЕро▓рпНрокро╛ ро▓рпЗрокрпНро╕рпН роЖродро╛ро░ рокроХрпБрокрпНрокро╛ропрпНро╡рпБ роЕроорпИрокрпНрокрпБ - роЗро░роХроЪро┐ропроорпН",
        'kn': "р▓Жр▓▓р│Нр▓лр▓╛ р▓▓р│Нр▓пр▓╛р▓мр│Нр▓╕р│Н р▓кр│Бр▓░р▓╛р▓╡р│Ж р▓╡р▓┐р▓╢р│Нр▓▓р│Зр▓╖р▓гр▓╛ р▓╡р│Нр▓пр▓╡р▓╕р│Нр▓ер│Ж - р▓Чр│Мр▓кр│Нр▓п"
    }
    canvas.drawString(doc.leftMargin, doc.bottomMargin, footer_texts.get(language, "Alfa Labs Evidence Analysis System - Confidential"))
    
    # Header text
    header_texts = {
        'en': "Automated Evidence Analysis Report",
        'ta': "родро╛ройро┐ропроЩрпНроХро┐ роЖродро╛ро░ рокроХрпБрокрпНрокро╛ропрпНро╡рпБ роЕро▒ро┐роХрпНроХрпИ",
        'kn': "р▓╕р│Нр▓╡р▓пр▓Вр▓Ър▓╛р▓▓р▓┐р▓д р▓кр│Бр▓░р▓╛р▓╡р│Ж р▓╡р▓┐р▓╢р│Нр▓▓р│Зр▓╖р▓гр▓╛ р▓╡р▓░р▓жр▓┐"
    }
    canvas.drawString(doc.leftMargin, doc.height + doc.topMargin - 12, header_texts.get(language, "Automated Evidence Analysis Report"))
    
    canvas.restoreState()

def _create_styles(language='en'):
    """Create comprehensive styling for the PDF with language support"""
    styles = getSampleStyleSheet()
    font_name = _register_fonts(language)
    
    # Custom styles with better typography and alignment
    title_style = ParagraphStyle(
        'Title',
        parent=styles['Heading1'],
        fontName=font_name,
        fontSize=16,
        spaceAfter=12,
        alignment=TA_CENTER,
        textColor=colors.HexColor('#2C5530'),
        spaceBefore=20
    )
    
    report_id_style = ParagraphStyle(
        'ReportID',
        parent=styles['Normal'],
        fontName=font_name,
        fontSize=9,
        spaceAfter=8,
        alignment=TA_CENTER,
        textColor=colors.HexColor('#666666'),
        backColor=colors.HexColor('#F8F9FA'),
        borderPadding=6,
        borderColor=colors.HexColor('#E9ECEF'),
        borderWidth=1
    )
    
    section_heading_style = ParagraphStyle(
        'SectionHeading',
        parent=styles['Heading2'],
        fontName=font_name,
        fontSize=11,
        spaceAfter=8,
        spaceBefore=12,
        textColor=colors.HexColor('#1E3A24'),
        leftIndent=0,
        borderLeft=3,
        borderLeftColor=colors.HexColor('#4A7C59'),
        borderLeftPadding=6,
        alignment=TA_LEFT
    )
    
    normal_style = ParagraphStyle(
        'Normal',
        parent=styles['Normal'],
        fontName=font_name,
        fontSize=9,
        spaceAfter=6,
        alignment=TA_JUSTIFY,
        leading=12,
        textColor=colors.HexColor('#333333'),
        wordWrap='LTR'
    )
    
    bullet_style = ParagraphStyle(
        'Bullet',
        parent=normal_style,
        leftIndent=15,
        spaceAfter=4,
        bulletIndent=5,
        firstLineIndent=-8,
        alignment=TA_LEFT
    )
    
    evidence_style = ParagraphStyle(
        'Evidence',
        parent=normal_style,
        backColor=colors.HexColor('#FFF3CD'),
        borderColor=colors.HexColor('#FFEaa6'),
        borderWidth=1,
        borderPadding=6,
        spaceAfter=8
    )
    
    footer_style = ParagraphStyle(
        'Footer',
        parent=styles['Normal'],
        fontName=font_name,
        fontSize=7,
        spaceAfter=4,
        alignment=TA_CENTER,
        textColor=colors.HexColor('#666666')
    )
    
    analytical_style = ParagraphStyle(
        'Analytical',
        parent=normal_style,
        backColor=colors.HexColor('#E8F5E8'),
        borderColor=colors.HexColor('#4CAF50'),
        borderWidth=1,
        borderPadding=8,
        spaceAfter=8
    )
    
    risk_high_style = ParagraphStyle(
        'RiskHigh',
        parent=normal_style,
        backColor=colors.HexColor('#FFEBEE'),
        borderColor=colors.HexColor('#F44336'),
        borderWidth=1,
        borderPadding=8,
        spaceAfter=8
    )
    
    risk_medium_style = ParagraphStyle(
        'RiskMedium',
        parent=normal_style,
        backColor=colors.HexColor('#FFF3E0'),
        borderColor=colors.HexColor('#FF9800'),
        borderWidth=1,
        borderPadding=8,
        spaceAfter=8
    )
    
    return {
        'title': title_style,
        'report_id': report_id_style,
        'section_heading': section_heading_style,
        'normal': normal_style,
        'bullet': bullet_style,
        'evidence': evidence_style,
        'footer': footer_style,
        'analytical': analytical_style,
        'risk_high': risk_high_style,
        'risk_medium': risk_medium_style
    }

def _create_header_table(report_id, language='en'):
    """Create a professional header table with language support"""
    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    texts = REPORT_TEXTS.get(language, REPORT_TEXTS['en'])
    
    header_data = [
        [texts['title'], f'Report ID: {report_id}'],
        [texts['header_system'], f'Generated: {current_time}'],
        [texts['header_police'], f'Classification: {texts["classification"]}']
    ]
    
    col_widths = [360, 180]
    
    header_table = Table(header_data, colWidths=col_widths)
    header_table.setStyle(TableStyle([
        ('BACKGROUND', (0, 0), (-1, -1), colors.HexColor('#2C5530')),
        ('TEXTCOLOR', (0, 0), (-1, -1), colors.whitesmoke),
        ('ALIGN', (0, 0), (0, -1), 'LEFT'),
        ('ALIGN', (1, 0), (1, -1), 'RIGHT'),
        ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),
        ('FONTNAME', (1, 0), (1, -1), 'Helvetica'),
        ('FONTSIZE', (0, 0), (-1, -1), 8),
        ('BOTTOMPADDING', (0, 0), (-1, -1), 4),
        ('TOPPADDING', (0, 0), (-1, -1), 4),
        ('LEFTPADDING', (0, 0), (-1, -1), 6),
        ('RIGHTPADDING', (0, 0), (-1, -1), 6),
        ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
    ]))
    
    return header_table

def _add_analytical_intelligence_section(content, enhanced_data, styles, texts):
    """Add analytical intelligence section to PDF"""
    content.append(Paragraph(texts['analytical_intelligence'], styles['section_heading']))
    
    analytical_data = enhanced_data.get('analytical_intelligence', {})
    risk_assessment = analytical_data.get('risk_assessment', {})
    confidence_metrics = analytical_data.get('confidence_metrics', {})
    insights = analytical_data.get('analytical_insights', [])
    
    # Risk Assessment
    if risk_assessment:
        risk_level = risk_assessment.get('risk_level', 'UNKNOWN')
        risk_style = styles['risk_high'] if risk_level == 'HIGH' else styles['risk_medium'] if risk_level == 'MEDIUM' else styles['analytical']
        
        risk_html = f"""
        <b>{texts['risk_assessment']}</b><br/>
        <b>{texts['threat_level']}:</b> {risk_level} (Score: {risk_assessment.get('risk_score', 0)}/100)<br/>
        <b>{texts['recommended_response']}:</b> {risk_assessment.get('recommended_response', 'Not specified')}<br/>
        <b>Risk Factors:</b> {', '.join(risk_assessment.get('risk_factors', []))}
        """
        content.append(Paragraph(risk_html, risk_style))
        content.append(Spacer(1, 0.1*inch))
    
    # Analytical Insights
    if insights:
        content.append(Paragraph(f"<b>{texts['analytical_insights']}</b>", styles['normal']))
        for insight in insights[:5]:  # Top 5 insights
            content.append(Paragraph(f"тАв {insight}", styles['bullet']))
        content.append(Spacer(1, 0.1*inch))
    
    # Confidence Metrics
    if confidence_metrics:
        content.append(Paragraph("<b>Detailed Confidence Metrics:</b>", styles['normal']))
        metrics_html = ""
        for metric, score in confidence_metrics.items():
            if metric != 'overall_confidence':
                percentage = f"{score * 100:.1f}%"
                metrics_html += f"<b>{metric.replace('_', ' ').title()}:</b> {percentage}<br/>"
        
        if metrics_html:
            content.append(Paragraph(metrics_html, styles['analytical']))
    
    content.append(Spacer(1, 0.15*inch))

def _add_visual_evidence_section(content, enhanced_data, styles, texts):
    """Add visual evidence section with key frames to PDF"""
    content.append(Paragraph(texts['visual_evidence'], styles['section_heading']))
    
    key_frames = enhanced_data.get('key_frames', [])
    
    if key_frames:
        content.append(Paragraph(f"<b>{texts['key_frames']}</b>", styles['normal']))
        
        for i, frame in enumerate(key_frames[:3], 1):  # Limit to 3 frames
            try:
                # Convert base64 to image
                frame_data = frame.get('image_data', '')
                if frame_data:
                    # Create temporary image file
                    import tempfile
                    with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as temp_file:
                        temp_file.write(base64.b64decode(frame_data))
                        temp_img_path = temp_file.name
                    
                    # Add frame description
                    timestamp = frame.get('timestamp_formatted', 'Unknown')
                    significance = frame.get('analytical_significance', 'Key moment')
                    
                    frame_desc = f"<b>Frame {i}</b> - {timestamp} - {significance}"
                    content.append(Paragraph(frame_desc, styles['normal']))
                    
                    # Add image to PDF
                    try:
                        img = Image(temp_img_path, width=3*inch, height=2*inch)
                        content.append(img)
                    except Exception as img_error:
                        content.append(Paragraph(f"[Visual Evidence: {timestamp} - Image load failed]", styles['evidence']))
                    
                    # Clean up
                    if os.path.exists(temp_img_path):
                        os.remove(temp_img_path)
                        
                    content.append(Spacer(1, 0.05*inch))
                    
            except Exception as e:
                logger.error(f"Error processing frame {i}: {e}")
                continue
    else:
        content.append(Paragraph("No key frames extracted for visual evidence.", styles['normal']))
    
    content.append(Spacer(1, 0.15*inch))

def _add_confidence_metrics_section(content, enhanced_data, styles, texts):
    """Add detailed confidence metrics section"""
    content.append(Paragraph(texts['confidence_metrics'], styles['section_heading']))
    
    confidence_metrics = enhanced_data.get('analytical_intelligence', {}).get('confidence_metrics', {})
    
    if confidence_metrics:
        # Create confidence metrics table
        metrics_data = [['Analysis Component', 'Confidence Score']]
        
        for metric, score in confidence_metrics.items():
            if metric != 'overall_confidence':
                metric_name = metric.replace('_', ' ').title()
                confidence_percent = f"{score * 100:.1f}%"
                metrics_data.append([metric_name, confidence_percent])
        
        if len(metrics_data) > 1:
            metrics_table = Table(metrics_data, colWidths=[3*inch, 1.5*inch])
            metrics_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#2C5530')),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, -1), 8),
                ('BOTTOMPADDING', (0, 0), (-1, -1), 6),
                ('BACKGROUND', (0, 1), (-1, -1), colors.HexColor('#F8F9FA')),
                ('GRID', (0, 0), (-1, -1), 1, colors.HexColor('#E9ECEF'))
            ]))
            content.append(metrics_table)
        
        # Overall confidence
        overall_conf = confidence_metrics.get('overall_confidence', 0)
        if overall_conf > 0:
            content.append(Spacer(1, 0.1*inch))
            conf_style = styles['analytical'] if overall_conf > 0.7 else styles['risk_medium'] if overall_conf > 0.5 else styles['risk_high']
            content.append(Paragraph(f"<b>Overall Analysis Confidence: {overall_conf * 100:.1f}%</b>", conf_style))
    else:
        content.append(Paragraph("Detailed confidence metrics not available for this analysis.", styles['normal']))
    
    content.append(Spacer(1, 0.15*inch))

def _enhance_analysis_with_intelligence(analysis_text, enhanced_data):
    """Enhance basic analysis with analytical intelligence"""
    enhanced_text = analysis_text
    
    # Add analytical insights if available
    analytical_insights = enhanced_data.get('analytical_intelligence', {}).get('analytical_insights', [])
    if analytical_insights:
        enhanced_text += "\n\nANALYTICAL INTELLIGENCE:\n"
        for insight in analytical_insights[:3]:
            enhanced_text += f"тАв {insight}\n"
    
    # Add risk assessment
    risk_assessment = enhanced_data.get('analytical_intelligence', {}).get('risk_assessment', {})
    if risk_assessment:
        enhanced_text += f"\nRISK ASSESSMENT: {risk_assessment.get('risk_level', 'UNKNOWN')} - {risk_assessment.get('recommended_response', '')}"
    
    return enhanced_text

def generate_pdf(analysis_text, report_id, language='en', enhanced_data=None):
    """Generate advanced PDF report with analytical intelligence and visual evidence"""
    # Validate language
    if language not in ['en', 'ta', 'kn']:
        language = 'en'
    
    # Register fonts and create styles for the selected language
    font_name = _register_fonts(language)
    styles = _create_styles(language)
    texts = REPORT_TEXTS.get(language, REPORT_TEXTS['en'])
    recommendations = RECOMMENDATIONS.get(language, RECOMMENDATIONS['en'])
    
    # Enhance analysis text with intelligence if enhanced data is available
    if enhanced_data:
        analysis_text = _enhance_analysis_with_intelligence(analysis_text, enhanced_data)
    
    # Create document with better margins
    buffer = io.BytesIO()
    doc = SimpleDocTemplate(
        buffer,
        pagesize=A4,
        rightMargin=0.5*inch,
        leftMargin=0.5*inch,
        topMargin=0.7*inch,
        bottomMargin=0.7*inch
    )
    
    # Create frame with watermark
    frame = Frame(doc.leftMargin, doc.bottomMargin, doc.width, doc.height, 
                  id='normal', showBoundary=0)
    
    def create_watermark(canvas, doc):
        return _create_watermark(canvas, doc, language)
    
    template = PageTemplate(id='watermarked', frames=[frame], onPage=create_watermark)
    doc.addPageTemplates([template])
    
    content = []
    
    # Header Table
    content.append(_create_header_table(report_id, language))
    content.append(Spacer(1, 0.2*inch))
    
    # Extract and structure AI analysis
    sections = _extract_ai_sections(analysis_text)
    
    # Case Information Section
    content.append(Paragraph(texts['case_info'], styles['section_heading']))
    case_info_html = f"""
    <b>{texts['case_number']}</b> <font color="#666666">______________________</font><br/>
    <b>{texts['investigating_officer']}</b> <font color="#666666">______________________</font><br/>
    <b>{texts['station']}</b> <font color="#666666">______________________</font><br/>
    <b>{texts['location_incident']}</b> <font color="#666666">______________________</font><br/>
    <b>{texts['date_incident']}</b> <font color="#666666">__/__/____</font>&nbsp;&nbsp;&nbsp;
    <b>{texts['time_incident']}</b> <font color="#666666">__:__</font>
    """
    content.append(Paragraph(case_info_html, styles['normal']))
    content.append(Spacer(1, 0.15*inch))
    
    # Executive Summary
    content.append(Paragraph(texts['executive_summary'], styles['section_heading']))
    executive_summary = sections.get('executive_summary', '')
    if not executive_summary:
        summary_paragraphs = _split_into_readable_paragraphs(analysis_text, 2)
        executive_summary = ' '.join(summary_paragraphs[:2])
    
    if executive_summary:
        content.append(Paragraph(_highlight_evidences(executive_summary), styles['evidence']))
    else:
        content.append(Paragraph(texts['no_summary'], styles['normal']))
    content.append(Spacer(1, 0.15*inch))
    
    # Detailed Evidence Analysis
    content.append(Paragraph(texts['detailed_analysis'], styles['section_heading']))
    detailed_analysis = sections.get('detailed_analysis', analysis_text)
    paragraphs = _split_into_readable_paragraphs(detailed_analysis)
    
    if paragraphs:
        for i, paragraph in enumerate(paragraphs[:6], 1):
            if paragraph and len(paragraph) > 20:
                highlighted_para = _highlight_evidences(paragraph)
                content.append(Paragraph(f"<b>{texts['analysis_point']} {i}:</b> {highlighted_para}", styles['normal']))
                content.append(Spacer(1, 0.08*inch))
    else:
        content.append(Paragraph(texts['no_analysis'], styles['normal']))
    
    # Add page break if content is getting long
    if len(paragraphs) > 3:
        content.append(PageBreak())
    
    content.append(Spacer(1, 0.15*inch))
    
    # Chronological Timeline
    content.append(Paragraph(texts['chronological_timeline'], styles['section_heading']))
    chronology_text = sections.get('chronology', analysis_text)
    chronology = _extract_chronology(chronology_text)
    
    if chronology:
        flow_items = []
        for i, entry in enumerate(chronology, 1):
            if len(entry) > 10:
                clean_entry = _highlight_evidences(entry.strip())
                flow_items.append(Paragraph(f"<b>{texts['event']} {i}:</b> {clean_entry}", styles['bullet']))
        
        if flow_items:
            chronology_list = ListFlowable(
                flow_items,
                bulletType='bullet',
                leftIndent=20,
                bulletOffsetY=2,
                spaceBefore=6,
                spaceAfter=6
            )
            content.append(chronology_list)
        else:
            content.append(Paragraph(texts['no_timeline'], styles['normal']))
    else:
        content.append(Paragraph(texts['no_timeline'], styles['normal']))
    
    content.append(Spacer(1, 0.15*inch))
    
    # Key Evidence Findings
    content.append(Paragraph(texts['key_evidence_findings'], styles['section_heading']))
    key_findings_text = sections.get('key_findings', analysis_text)
    key_observations = _extract_key_observations(key_findings_text)
    
    if key_observations:
        for i, observation in enumerate(key_observations, 1):
            if len(observation) > 15:
                highlighted_obs = _highlight_evidences(observation)
                content.append(Paragraph(f"<b>{texts['finding']} {i}:</b> {highlighted_obs}", styles['bullet']))
                content.append(Spacer(1, 0.04*inch))
    else:
        content.append(Paragraph(texts['no_findings'], styles['normal']))
    
    content.append(Spacer(1, 0.15*inch))
    
    # Critical Identifiers
    content.append(Paragraph(texts['critical_identifiers'], styles['section_heading']))
    
    plate_pattern = re.compile(r'\b(TN[-\s]?\d{1,2}[-\s]?[A-Z]{1,2}[-\s]?\d{1,4})\b', re.IGNORECASE)
    phone_pattern = re.compile(r'\b(\d{10}|\d{3}[-.\s]??\d{3}[-.\s]??\d{4})\b')
    
    plates = plate_pattern.findall(analysis_text)
    phones = phone_pattern.findall(analysis_text)
    
    identifiers_html = ""
    
    if plates:
        unique_plates = list(set(plates))
        plates_text = ", ".join([f'<font color="#D32F2F"><b>{plate}</b></font>' for plate in unique_plates[:5]])
        identifiers_html += f"<b>{texts['vehicle_plates']}</b> {plates_text}<br/>"
    
    if phones:
        unique_phones = list(set(phones))
        phones_text = ", ".join([f'<font color="#388E3C"><b>{phone}</b></font>' for phone in unique_phones[:5]])
        identifiers_html += f"<b>{texts['phone_numbers']}</b> {phones_text}<br/>"
    
    if identifiers_html:
        content.append(Paragraph(identifiers_html, styles['normal']))
    else:
        content.append(Paragraph(texts['no_identifiers'], styles['normal']))
    
    content.append(Spacer(1, 0.15*inch))
    
    # NEW: Analytical Intelligence Section
    if enhanced_data:
        _add_analytical_intelligence_section(content, enhanced_data, styles, texts)
    
    # NEW: Visual Evidence Section
    if enhanced_data:
        _add_visual_evidence_section(content, enhanced_data, styles, texts)
    
    # NEW: Confidence Metrics Section
    if enhanced_data:
        _add_confidence_metrics_section(content, enhanced_data, styles, texts)
    
    # Investigative Recommendations
    content.append(Paragraph(texts['investigative_recommendations'], styles['section_heading']))
    
    for rec in recommendations:
        content.append(Paragraph(f"тАв {rec}", styles['bullet']))
        content.append(Spacer(1, 0.03*inch))
    
    content.append(Spacer(1, 0.2*inch))
    
    # Footer Section
    footer_html = f"""
    <b>{texts['report_generated_by']}</b> Alfa Labs Automated Evidence Analysis System<br/>
    <b>{texts['system_version']}</b> 2.1 | <b>{texts['analysis_confidence']}</b> High<br/>
    <i>{texts['computer_generated_note']}</i>
    """
    content.append(Paragraph(footer_html, styles['footer']))
    content.append(Spacer(1, 0.08*inch))
    
    # Signature Section
    signature_html = f"""
    <b>{texts['investigating_officer_signature']}</b> __________________________ &nbsp;&nbsp;&nbsp;&nbsp; 
    <b>{texts['date']}</b> __/__/____<br/>
    <b>{texts['reviewing_officer_signature']}</b> __________________________ &nbsp;&nbsp;&nbsp;&nbsp; 
    <b>{texts['date']}</b> __/__/____
    """
    content.append(Paragraph(signature_html, styles['footer']))
    
    # Build PDF
    try:
        doc.build(content)
        buffer.seek(0)
        return buffer.getvalue()
    except Exception as e:
        print(f"PDF generation error: {e}")
        return _generate_fallback_pdf(report_id, language)

def _generate_fallback_pdf(report_id, language='en'):
    """Generate a fallback PDF in case of errors"""
    buffer = io.BytesIO()
    doc = SimpleDocTemplate(buffer, pagesize=A4)
    styles = _create_styles(language)
    texts = REPORT_TEXTS.get(language, REPORT_TEXTS['en'])
    
    content = []
    content.append(Paragraph(texts['title'], styles['title']))
    content.append(Paragraph(f"Report ID: {report_id}", styles['report_id']))
    content.append(Spacer(1, 0.3*inch))
    content.append(Paragraph("Error generating detailed report. Please check the system logs.", styles['normal']))
    
    doc.build(content)
    buffer.seek(0)
    return buffer.getvalue()

# For backward compatibility
if __name__ == "__main__":
    # Test function with comprehensive sample text
    test_text = """
    Evidence analysis shows a vehicle with plate TN-09-AB-1234 was observed at 14:30 near Chennai Central.
    A suspicious individual was seen near the location at 14:45. The vehicle then moved towards Coimbatore.
    Another sighting occurred at 16:00 in Madurai. Phone number 9876543210 was mentioned in connection.
    
    Key findings include potential illegal activity observed near the vehicle. The suspect appeared to be 
    carrying a suspicious package. The incident occurred during daylight hours with clear visibility.
    
    Timeline of events:
    14:30 - Vehicle first observed at Chennai Central
    14:45 - Suspicious individual approaches vehicle  
    15:20 - Vehicle departs towards Coimbatore
    16:00 - Second sighting in Madurai
    
    Important evidence includes the vehicle details and the suspicious package observed.
    """
    
    # Test all languages
    for lang in ['en', 'ta', 'kn']:
        pdf_data = generate_pdf(test_text, f"TEST-2024-001-{lang}", lang)
        with open(f"test_report_{lang}.pdf", "wb") as f:
            f.write(pdf_data)
        print(f"PDF generated successfully for {lang}!")